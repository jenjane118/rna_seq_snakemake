{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGS Analysis of Mbovis Samples from Algeria\n",
    "\n",
    "## Jennifer J. Stiens\n",
    "### j.j.stiens@gmail.com\n",
    "\n",
    "### Project start: 8 December 2022\n",
    "\n",
    "First step is variant calling in these samples vs reference strain. Secondly, identifying a clade? How do these strains fit in the phylogeny of Mbovis? \n",
    "\n",
    "Are these different samples of one strain? Taxon ID '233413' which is Mbovis AF2122/97, or thought to be different strains?\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "[Zimpel et al, 2020](https://www.frontiersin.org/article/10.3389/fmicb.2020.00843)\n",
    "\n",
    "[Loiseau et al, 2020](https://pubmed.ncbi.nlm.nih.gov/32211193/)\n",
    "\n",
    "[Zimpel et al, 2017](https://www.frontiersin.org/articles/10.3389/fmicb.2017.02389/full)\n",
    "\n",
    "[Reis and Cunha, 2021](https://doi.org/10.1038/s41598-021-98226-y)\n",
    "\n",
    "[Branger et al, 2020](https://pubmed.ncbi.nlm.nih.gov/32240800/)\n",
    "\n",
    "[Zwyer et al, 2021](https://s3-eu-west-1.amazonaws.com/openreseurope/manuscripts/15458/2094db28-ac1b-41b6-9bca-bd3e5e6b28ad_14029_-_daniela_brites_v2.pdf?doi=10.12688/openreseurope.14029.2&numberOfBrowsableCollections=88&numberOfBrowsableInstitutionalCollections=0&numberOfBrowsableGateways=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "Refer to Zimpel pipeline: \n",
    "[Zimpel snp pipeline](https://github.com/LaPAM-USP/Zimpel-2019/blob/master/Snps_pipeline)\n",
    "\n",
    "USDA-VS pipeline used by Reis and Cunha:\n",
    "[vSNP3](https://github.com/USDA-VS/vSNP3)\n",
    "This is a conda python pipeline that \"generates BAM, VCF and annotated SNP tables and corresponding phylogenetic trees to achieve a high resolution SNP analysis\" [user manual](https://github.com/USDA-VS/vSNP/blob/master/docs/detailed_usage.md)\n",
    "\n",
    "Irilenia's pipeline from 'omics practical\n",
    "[genomeComparison_practical](file:///Users/jenniferstiens/Documents/omics_demo/GenomeComparison_practical_IN.html)\n",
    "Uses freebayes to create VCF file\n",
    "\n",
    "Zwyer pipeline using kvarQ\n",
    "[kvarq testsuite](https://github.com/gagneux-lab/LivestockAssociatedMTBC/blob/main/KvarQ_testsuite/MTBC_animals/phylo.py)\n",
    "\n",
    "\n",
    "\n",
    "### Basic steps\n",
    "- Trim reads, remove duplicates and check quality (with fastp)\n",
    "- Map to reference genome (*with bwa-mem)\n",
    "- Picard to remove duplicates in mapped reads??\n",
    "- find snps and generate vcf file (with samtools mpileup/varscan, or freebayes/vcftools)\n",
    "- create consensus file and annotate (vcflib(vcf2fastq)/DFAST or snpEFF)\n",
    "- remove repetitive genes and indels from analysis?\n",
    "- compare to snp markers to determine lineage\n",
    "\n",
    "\n",
    "### Facts to keep in mind:\n",
    "\n",
    "SNP mutations are rare in bacterial species (typically around 1 mutation per 1 billion replications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# download files from AWS (downloaded list of links from microbesNG)\n",
    "wget -i ~/myco_projects/WGS_variants_bovis/links_microbesng-data_amazonaws.txt\n",
    "\n",
    "# change sample names to begin with 'dna'\n",
    "cd fastqs\n",
    "for FILENAME in *; do mv $FILENAME dna_$FILENAME; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# run fastp to trim, remove duplicates and check quality\n",
    "# use snakefile for fastp\n",
    "\n",
    "cd /Volumes/Data_disk/Mbovis_wgs\n",
    "conda activate snakemake\n",
    "snakemake -np -s ~/snakemake/fastp/pe/snakefile.smk\n",
    "nohup snakemake --cores 2 -s ~/snakemake/fastp/pe/snakefile.smk > nohup.out 2>&1 &\n",
    "\n",
    "# run quality control with fastqc\n",
    "nohup snakemake --cores 2 -s ~/snakemake/fastqc/pe/snakefile.smk > nohup.out 2>&1 &\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#still having trouble completing fastqc without timing out\n",
    "#added profile (see https://www.embl.org/groups/bioinformatics-rome/blog/2022/03/snakemake-profile-2-reducing-command-line-options-with-profile/)\n",
    "\n",
    "vi ~/snakemake/profile/config.yaml\n",
    "\n",
    "---\n",
    "\n",
    "cores: 2\n",
    "latency-wait: 60\n",
    "reason: True\n",
    "show-failed-logs: True\n",
    "keep-going: True\n",
    "printshellcmds: True\n",
    "rerun-incomplete: True\n",
    "restart-times: 3\n",
    "\n",
    "# will finish but very slow as continually restarting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "snakemake -np --profile ~/snakemake/profile/.  -s ~/snakemake/fastqc/pe/snakefile.smk\n",
    "nohup snakemake --profile ~/snakemake/profile/.  -s ~/snakemake/fastqc/pe/snakefile.smk > nohup.out 2>&1 &\n",
    "cd fastqc_outputs\n",
    "multiqc ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All samples between 0.7-1.3 M reads, except for A34 which is 0.3M reads (each 1 and 2)\n",
    "\n",
    "## Map to Reference Genome \n",
    "Mycobacterium tuberculosis variant bovis AF2122/97, NC_002945.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#first need to create index files (using bwa) for reference genome (in same directory)\n",
    "bwa index ~/myco_projects/ref_seqs/Mbovis/Mbovis_AF212297.fasta\n",
    "\n",
    "snakemake -np --profile ~/snakemake/profile/. -s ~/snakemake/map_bwa/pe/snakefile.smk\n",
    "nohup snakemake --profile ~/snakemake/profile/. -s ~/snakemake/map_bwa/pe/snakefile.smk > nohup_map.out 2>&1 &\n",
    "\n",
    "cd sorted_reads\n",
    "multiqc ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All but sample 35 have > 1M reads mapped\n",
    "\n",
    "\n",
    "From Zimpel et al Snps pipeline (https://github.com/LaPAM-USP/Zimpel-2019/blob/master/Snps_pipeline):\n",
    "\n",
    "#check depth in RDs positions (developed by Robson Francisco de Souza):\n",
    "\\ls *.bam | parallel --citation -N1 -j5 \"samtools depth {} | awk '\\$2 >= #POSITION1NITIAL# && \\$2 <= #POSITION2FINAL#{a+=\\$3;n++}END{print \\\"{.}\\t\\\",a/n}'\"\n",
    "This relies on coverage of base positions, so need to run bam coverage, or the like?\n",
    "\n",
    "\n",
    "#Picard\n",
    "for i in `ls *.bam`:\n",
    "    do picard MarkDuplicates REMOVE_DUPLICATES=true INPUT=$i OUTPUT=$i.dupl.bam METRICS_FILE=$i.txt\n",
    "done\n",
    "\n",
    "samtools view input.bam.dupl.bam | wc -l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Create conda env or add to another env\n",
    "conda create --name dna python=3.8\n",
    "conda activate dna  #or conda activate snakemake\n",
    "conda install -c bioconda picard varscan freebayes vcftools snpeff\n",
    "\n",
    "\n",
    "for i in `ls *.bam`:\n",
    "    do picard MarkDuplicates REMOVE_DUPLICATES=true INPUT=$i OUTPUT=$i.dupl.bam METRICS_FILE=$i.txt\n",
    "done\n",
    "\n",
    "#new syntax warning:\n",
    "#MarkDuplicates -REMOVE_DUPLICATES true -INPUT dna_236574_A20.bam -OUTPUT dna_236574_A20.bam.dupl.bam -METRICS_FILE dna_236574_A20.bam.txt\n",
    "\n",
    "#takes a few minutes for 10 samples, but not too bad. Create new snakemake file or add to mapping etc?\n",
    "\n",
    "#count reads after removing dups\n",
    "for i in `ls *.dupl.bam`; do samtools view ${i} | wc -l; done\n",
    "# dna_236574_A20.bam.dupl.bam   1188622\n",
    "#  dna_236575_A25.bam.dupl.bam  1668956\n",
    "#  dna_236576_A26.bam.dupl.bam  1361006\n",
    "#  dna_236577_A27.bam.dupl.bam  1234336\n",
    "#  dna_236578_A29.bam.dupl.bam  1272475\n",
    "#  dna_236579_A30.bam.dupl.bam  1353532\n",
    "#  dna_236580_A31.bam.dupl.bam  1277324\n",
    "#  dna_236581_A32.bam.dupl.bam  1452253\n",
    "#  dna_236582_A34.bam.dupl.bam   592629\n",
    "#  dna_236583_A35.bam.dupl.bam  1898698\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Find snps\n",
    "#Samtools mpileup\n",
    "\n",
    "#mpileup has replaced pileup in samtools for both multiple and single samples\n",
    "\n",
    "#-q = min mapping quality; -Q = min base quality\n",
    "cd sorted_reads\n",
    "for i in $(ls *.dupl.bam); do samtools mpileup -f /Users/jenniferstiens/myco_projects/ref_seqs/Mbovis/Mbovis_AF212297.fasta -q 20 -Q 20 \"$i\" > \"$i\".mpileup; done\n",
    "\n",
    "#takes some minutes\n",
    "\n",
    "#move .mpileup files to vcf_files directory\n",
    "mv *.mpileup /Volumes/Data_disk/Mbovis_wgs/vcf_files/\n",
    "\n",
    "\n",
    "# test one file to create vcf with freebayes and with varscan\n",
    "#freebayes (uses fasta files)\n",
    "cd sorted_reads\n",
    "freebayes -f  /Users/jenniferstiens/myco_projects/ref_seqs/Mbovis/Mbovis_AF212297.fasta -p 1  dna_236574_A20.bam > dna_236574_A20.vcffile\n",
    "\n",
    "#varscan (uses mpileup files)\n",
    "cd /Volumes/Data_disk/Mbovis_wgs/vcf_files\n",
    "varscan mpileup2cns dna_236574_A20.bam.dupl.bam.mpileup --min-coverage 7 --min-var-freq 0.1 --min-freq-for-hom 0.90 --variants --output-vcf 1 > dna_236574_A20.bam.dupl.bam.mpileup.varscan.vcf\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample lines from freebayes vcf file:\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tunknown\n",
    "NC_002945.4\t389\t.\tA\tC\t2.26038e-15\t.\tAB=0;ABP=0;AC=0;AF=0;AN=1;AO=7;CIGAR=1X;DP=121;DPB=121;DPRA=0;EPP=10.7656;EPPR=4.56684;GTI=0;LEN=1;MEANALT=2;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=734.955;PAIRED=0.714286;PAIREDR=0.654867;PAO=0;PQA=0;PQR=0;PRO=0;QA=117;QR=3685;RO=113;RPL=3;RPP=3.32051;RPPR=8.56389;RPR=4;RUN=1;SAF=4;SAP=3.32051;SAR=3;SRF=51;SRP=5.3355;SRR=62;TYPE=snp\tGT:DP:AD:RO:QR:AO:QA:GL\t0:121:113,7:113:3685:7:117:0,-321.113\n",
    "NC_002945.4\t835\t.\tA\tT\t0\t.\tAB=0;ABP=0;AC=0;AF=0;AN=1;AO=6;CIGAR=1X;DP=115;DPB=115;DPRA=0;EPP=3.0103;EPPR=11.7958;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=742.237;PAIRED=0.833333;PAIREDR=0.981651;PAO=0;PQA=0;PQR=0;PRO=0;QA=99;QR=3715;RO=109;RPL=2;RPP=4.45795;RPPR=3.50834;RPR=4;RUN=1;SAF=3;SAP=3.0103;SAR=3;SRF=47;SRP=7.4927;SRR=62;TYPE=snp\tGT:DP:AD:RO:QR:AO:QA:GL\t0:115:109,6:109:3715:6:99:0,-325.433\n",
    "NC_002945.4\t1057\t.\tA\tG\t3465.73\t.\tAB=0;ABP=0;AC=1;AF=1;AN=1;AO=110;CIGAR=1X;DP=111;DPB=111;DPRA=0;EPP=9.40627;EPPR=0;GTI=0;LEN=1;MEANALT=2;MQM=60;MQMR=0;NS=1;NUMALT=1;ODDS=798.014;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=3921;QR=0;RO=0;RPL=58;RPP=3.72096;RPPR=0;RPR=52;RUN=1;SAF=72;SAP=25.8305;SAR=38;SRF=0;SRP=0;SRR=0;TYPE=snp\tGT:DP:AD:RO:QR:AO:QA:GL\t1:111:0,110:0:0:110:3921:-352.938,0\n",
    "\n",
    "no filtering as set. Can use vcf tools to filter out low quality variants:\n",
    "\n",
    "[freebayes and vcftools](https://hbctraining.github.io/In-depth-NGS-Data-Analysis-Course/sessionVI/lessons/02_variant-calling.html)\n",
    "\n",
    "[vcf tools](https://vcftools.github.io/man_latest.html)\n",
    "\n",
    "NOt sure about difference in how tools work--freebayes is evidently not 'alignment-based' \n",
    "\n",
    "[freebayes github](https://github.com/freebayes/freebayes)\n",
    "\n",
    "Get a lot more feedback on std output with varscan mpileup2cns \n",
    "\"This command makes consensus calls (SNP/Indel/Reference) from a mpileup file based on user-defined parameters\"\n",
    "OPTIONS:\n",
    "\t--min-coverage\tMinimum read depth at a position to make a call [8]\n",
    "\t--min-reads2\tMinimum supporting reads at a position to call variants [2]\n",
    "\t--min-avg-qual\tMinimum base quality at a position to count a read [15]\n",
    "\t--min-var-freq\tMinimum variant allele frequency threshold [0.01]\n",
    "\t--min-freq-for-hom\tMinimum frequency to call homozygote [0.75]\n",
    "\t--p-value\tDefault p-value threshold for calling variants [99e-02]\n",
    "\t--strand-filter\tIgnore variants with >90% support on one strand [1]\n",
    "\t--output-vcf\tIf set to 1, outputs in VCF format\n",
    "\t--variants\tReport only variant (SNP/indel) positions (mpileup2cns only) [0]\n",
    "\n",
    "\n",
    "\n",
    "Example output on one file:\n",
    "\n",
    "Only variants will be reported\n",
    "Warning: No p-value threshold provided, so p-values will not be calculated\n",
    "Min coverage:\t7\n",
    "Min reads2:\t2\n",
    "Min var freq:\t0.1\n",
    "Min avg qual:\t15\n",
    "P-value thresh:\t0.01\n",
    "Reading input from dna_236574_A20.bam.dupl.bam.mpileup\n",
    "4336824 bases in pileup file\n",
    "816 variant positions (739 SNP, 77 indel)\n",
    "21 were failed by the strand-filter\n",
    "795 variant positions reported (719 SNP, 76 indel)\n",
    "\n",
    "\n",
    "lines from file:\n",
    "\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tSample1\n",
    "NC_002945.4\t1057\t.\tA\tG\t.\tPASS\tADP=84;WT=0;HET=0;HOM=1;NC=0\tGT:GQ:SDP:DP:RD:AD:FREQ:PVAL:RBQ:ABQ:RDF:RDR:ADF:ADR\t1/1:255:84:84:0:84:100%:4.3483E-50:0:36:0:0:52:32\n",
    "NC_002945.4\t4480\t.\tT\tC\t.\tPASS\tADP=72;WT=0;HET=0;HOM=1;NC=0\tGT:GQ:SDP:DP:RD:AD:FREQ:PVAL:RBQ:ABQ:RDF:RDR:ADF:ADR\t1/1:255:72:72:0:72:100%:6.7558E-43:0:36:0:0:43:29\n",
    "\n",
    "on varscan manual page:\n",
    "[varscan manual](https://varscan.sourceforge.net/using-varscan.html#v2.3_pileup2snp)\n",
    "Note, to save disk space and file I/O, you can redirect mpileup output directly to VarScan with a \"pipe\" command. For example: \n",
    "samtools mpileup -f reference.fasta myData.bam | java -jar VarScan.v2.2.jar pileup2snp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#varscan\n",
    "\n",
    "cd /Volumes/Data_disk/Mbovis_wgs/vcf_files\n",
    "\n",
    "# loop through mpileup files\n",
    "for file in *.bam.dupl.bam.mpileup; do varscan mpileup2cns ${file} --min-coverage 7 --min-var-freq 0.1 --min-freq-for-hom 0.90 --variants --output-vcf 1 > ${file}.varscan.vcf; done\n",
    "\n",
    "# annotate variant files with snpEff\n",
    "\n",
    "# find available databases\n",
    "snpEff databases | grep -i mycobacterium_bovis_af2122_97\n",
    "\n",
    "#Mycobacterium_bovis_af2122_97                               \tMycobacterium_bovis_af2122_97                               \t          \t                              \t\n",
    "#[https://snpeff.blob.core.windows.net/databases/v5_1/snpEff_v5_1_Mycobacterium_bovis_af2122_97.zip, \n",
    "#https://snpeff.blob.core.windows.net/databases/v5_0/snpEff_v5_0_Mycobacterium_bovis_af2122_97.zip]\n",
    "#Mycobacterium_bovis_af2122_97_gca_000195835                 \tMycobacterium_bovis_af2122_97_gca_000195835\n",
    "# when I looked at the second in ncbi, it said 'assembly has been supressed'\n",
    "#https://www.ncbi.nlm.nih.gov/assembly/GCF_000195835.2/\n",
    "#                 \t          \t                              \t[https://snpeff.blob.core.windows.net/databases/v5_1/snpEff_v5_1_Mycobacterium_bovis_af2122_97_gca_000195835.zip, https://snpeff.blob.core.windows.net/databases/v5_0/snpEff_v5_0_Mycobacterium_bovis_af2122_97_gca_000195835.zip]\n",
    "\n",
    "cd /Volumes/Data_disk/Mbovis_wgs/vcf_files/\n",
    "snpEff Mycobacterium_bovis_af2122_97 dna_236574_A20.bam.dupl.bam.mpileup.varscan.vcf > ann_vcf/dna_236574_A20.ann.vcf\n",
    "\n",
    "#00:00:00 ERROR while connecting to https://snpeff.blob.core.windows.net/databases/v5_1/snpEff_v5_1_Mycobacterium_bovis_af2122_97.zip\n",
    "# all say ERROR_CHROMOSOME_NOT_FOUND\n",
    "\n",
    "#replace Chromosome name in vcf files so it can be used in snpEff\n",
    "cd mbovis_vcf_files\n",
    "for f in $(ls *.vcf)\n",
    "do\n",
    "    sed -i '' 's/NC_002945.4/Chromosome/g' \"$f\" > \"$i\".vcf  #extra empty string for mac\n",
    "        echo \"Processing $f\"\n",
    "done\n",
    "\n",
    "snpEff -v -no-downstream -no-upstream Mycobacterium_bovis_af2122_97 mbovis_vcf_files/dna_236574_A20.bam.dupl.bam.mpileup.varscan.vcf > mbovis_vcf_files/test.annot.vcf\n",
    "\n",
    "#use snakefile\n",
    "#need to change names of files to match more generic for snakefile\n",
    "for file in *\n",
    "do\n",
    "base=`basename $file .bam.dupl.bam`\n",
    "mv $file $base.dupl.bam\n",
    "done\n",
    "\n",
    "# do this as go along in snakefile \n",
    "#snakemake -np --profile ~/snakemake/profile/. -s ~/snakemake/variant_calling/snakefile.smk\n",
    "#nohup snakemake --profile ~/snakemake/profile/. -s ~/snakemake/variant_calling/snakefile.smk > nohup.out 2>&1 &\n",
    "\n",
    "#this didn't work ERROR 'Does not match genome' and first base is always 'N', problem starts with mpileup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re map to H37Rv (do not do)\n",
    "\n",
    "In zimpel paper, used H37Rv since bovis is missing no genes from Mtb. But this means i need to use H37Rv for mpileup. AND re-map using Mtb instead of Mbovis. **Correction, they used mbovis for snpEFF, see github https://github.com/LaPAM-USP/Zimpel-2019/blob/master/Snps_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#redo all mapping/mpileup/varscan with H37Rv. \n",
    "\n",
    "#Re-mapped with mapping snakefile. Added picard de-duplicating to snakefile\n",
    "\n",
    "snakemake -np --profile ~/snakemake/profile/. -s ~/snakemake/map_bwa/pe/snakefile.smk\n",
    "nohup snakemake --profile ~/snakemake/profile/. -s ~/snakemake/map_bwa/pe/snakefile.smk > nohup_map.out 2>&1 &\n",
    "\n",
    "\n",
    "#make new snakefile and use for mpileup and varscan (and eventually snpeff)\n",
    "#snakemake/variant_calling/snakefile.smk\n",
    "\n",
    "snakemake -np --profile ~/snakemake/profile/. -s ~/snakemake/variant_calling/snakefile.smk\n",
    "nohup snakemake --profile ~/snakemake/profile/. -s ~/snakemake/variant_calling/snakefile.smk > nohup.out 2>&1 &\n",
    "\n",
    "#getting errors trying to run samtools mpileup with the H37Rv reference. re index with samtools?\n",
    "samtools mpileup -f /Users/jenniferstiens/myco_projects/ref_seqs/Mtb/AL123456.3.fasta -q 20 -Q 20 deduped_sorted_reads/dna_236574_A20.dupl.bam > dna_236574_A20.mpileup\n",
    "samtools faidx ~/myco_projects/ref_seqs/Mtb/AL123456.3.fasta\n",
    "# this did the trick. need different indexes for bwa and for samtools mpileup?\n",
    "\n",
    "#snpEFF\n",
    "\n",
    "#check Chromosome name (this was probably the same issue above with Mbovis mapped?)\n",
    "awk '{print$1}' myfile.vcf | tail -n 1\n",
    "#AL123456.3\n",
    "\n",
    "#replace Chromosome name in vcf files so it can be used in snpEff\n",
    "for f in $(ls *.vcf)\n",
    "do\n",
    "    sed -i 's/AL123456.3/Chromosome/g' \"$f\" > \"$i\".vcf\n",
    "        echo \"Processing $f\"\n",
    "done\n",
    "\n",
    "# do do one at a time with pipe:\n",
    "sed 's/AL123456.3/Chromosome/g' mtb_vcf_files/dna_236575_A25.varscan.vcf | snpEff -v -no-downstream -no-upstream Mycobacterium_tuberculosis_h37rv > test.annot.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume these are different strains rather than different samples from same strain and run each separately. Want to spoligotype the genomes as well. there are in silico measures to find spoligotype patterns https://github.com/kvarq/kvarq and https://github.com/xiaeryu/SpoTyping-v2.0. Then you can use these patterns in Mbovis spoligotype database https://www.mbovis.org to assign names to spoligotypes. (look at Zwyer paper)\n",
    "\n",
    "Downloaded spreadsheet of SNPs and associated lineages from Zimpel et al, 2020. Maybe can use this to compare position and allele and identify common snps with my WGS data?\n",
    "\n",
    "https://github.com/gagneux-lab/LivestockAssociatedMTBC/blob/main/KvarQ_testsuite/MTBC_animals/phylo.py\n",
    "\n",
    "From this paper: https://open-research-europe.ec.europa.eu/articles/1-100\n",
    "\n",
    "\"In the cases for which WGS exist, sequencing reads can be queried with a new suite of markers (See Zenodo reposi- tory), using KvarQ and bypassing the need to run conventional alignment approaches and phylogenetic analysis for strain classification.‚Äù\n",
    "\n",
    "\n",
    "Can't install kvarq on my laptop--Dave H has installed on thoth server\n",
    "\n",
    "\n",
    "Downloaded spreadsheet of SNPs, could do manually if kvarq doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#to use kvarq on thoth\n",
    "#don't log into bash shell or activate conda env since need different python version loaded\n",
    "\n",
    "module use -a /s/software/modules\n",
    "module load python/v2\n",
    "conda activate kvarq\n",
    "kvarq --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kvarq on thoth\n",
    "\n",
    "Download sequences (fastq) from aws to thoth using wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#transfer list of aws links\n",
    "scp ~/myco_projects/WGS_variants_bovis/links_microbesng-data_amazonaws.txt sj003@ssh.cryst.bbk.ac.uk:/d/in16/u/sj003/mbovis_wgs/fastq\n",
    "cd /d/in16/u/sj003/mbovis_wgs/fastq\n",
    "# download files from AWS (downloaded list of links from microbesNG)\n",
    "wget -i links_microbesng-data_amazonaws.txt\n",
    "\n",
    "#move testsuite from zwyer to new testsuites directory (all files need to be there)\n",
    "mkdir testsuites\n",
    "scp ~/PycharmProjects/phylogeny/_util.py sj003@ssh.cryst.bbk.ac.uk:/d/in16/u/sj003/mbovis_wgs/testsuites\n",
    "scp ~/PycharmProjects/phylogeny/phylo.py sj003@ssh.cryst.bbk.ac.uk:/d/in16/u/sj003/mbovis_wgs/testsuites\n",
    "scp ~/Desktop/MTB_ancestor_reference.bases sj003@ssh.cryst.bbk.ac.uk:/d/in16/u/sj003/mbovis_wgs/testsuites/\n",
    "\n",
    "\n",
    "#scan file with kvarq (creates .json coverages file)\n",
    "kvarq scan -l MTBC -p fastq/dna_236574_A20_1.fastq.gz dna_236574_A20_1.json\n",
    "\n",
    "#load testsuite *must just indicate working directory and then name of path separately\n",
    "kvarq -t . info -l testsuites/phylo\n",
    "\n",
    "#check gui\n",
    "kvarq gui\n",
    "# testsuite doesn't show up on gui, restricted to command line\n",
    "\n",
    "#scan with zwyer testsuite\n",
    "kvarq -t . scan -l testsuites/phylo fastq/dna_236574_A20_1.fastq.gz dna_236574_A20_1.json\n",
    "\n",
    "#make loop to scan each file\n",
    "# run_kvarq.sh\n",
    "#!/bin/bash\n",
    "for file in fastq/*\n",
    "do\n",
    "        base=`basename $file .fastq.gz`\n",
    "        kvarq -t . scan -l testsuites/phylo $file $base.json\n",
    "done\n",
    "\n",
    "chmod +xr run_kvarq.sh\n",
    "bash run_kvarq.sh\n",
    "\n",
    "# takes a few minutes for each, may want to run in background in future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these files: \n",
    "fastq/dna_236579_A30_1.fastq.gz, fastq/dna_236579_A30_2.fastq.gz \n",
    "fastq/dna_236581_A32_1.fastq.gz, fastq/dna_236581_A32_2.fastq.gz\n",
    "\n",
    "got the following error:\n",
    "    \"cannot read next deflated stream in compressed file : expected method==DEFLATED\"\n",
    "    \"could not scan fastq/dna_236581_A32_2.fastq.gz : error while inflating compressed data\"\n",
    "\n",
    "Still produced .json files for A30, but not A32\n",
    "\n",
    "Decompressed file ahead of time and then run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gunzip fastq/dna_236581_A32_1.fastq.gz fastq/dna_236581_A32_2.fastq.gz\n",
    "kvarq -t . scan -l testsuites/phylo fastq/dna_236581_A32_1.fastq fastq/dna_236581_A32_1.json\n",
    "kvarq -t . scan -l testsuites/phylo fastq/dna_236581_A32_2.fastq fastq/dna_236581_A32_2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes a coverage file and separate analysis for each one of each pair. \n",
    "Parsed json files to get lineage assignment in R 'mbovis_wgs.Rmd'\n",
    "\n",
    "fastq/dna_236574_A20_1.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236574_A20_2.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236575_A25_1.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236575_A25_2.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236576_A26_1.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236576_A26_2.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236577_A27_1.fastq.gz\tLa1/La1.7-La1.7.1 -- mixed coverage\n",
    "fastq/dna_236577_A27_2.fastq.gz\tLa1/La1.7-La1.7.1 -- mixed coverage\n",
    "fastq/dna_236578_A29_1.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236578_A29_2.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236579_A30_1.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236579_A30_2.fastq.gz\tLa1/La1.8-La1.8.2\n",
    "fastq/dna_236580_A31_1.fastq.gz\tLa1/La1.2 -- mixed coverage\n",
    "fastq/dna_236580_A31_2.fastq.gz\tLa1/La1.2 -- mixed coverage\n",
    "fastq/dna_236581_A32_1.fastq\tLa1/La1.8-La1.8.2 -- mixed coverage\n",
    "fastq/dna_236581_A32_2.fastq\tLa1/La1.8-La1.8.2 -- mixed coverage\n",
    "fastq/dna_236582_A34_1.fastq.gz\tLa1/La1.7-La1.7.1 -- mixed coverage\n",
    "fastq/dna_236582_A34_2.fastq.gz\tLa1/La1.7 -- low coverage (median below 10x) -- mixed coverage\n",
    "fastq/dna_236583_A35_1.fastq.gz\tLa1/La1.7-La1.7.1 -- mixed coverage\n",
    "fastq/dna_236583_A35_2.fastq.gz\tLa1/La1.7-La1.7.1 -- mixed coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(here)\n",
    "library(tidyverse)\n",
    "library(jsonlite)\n",
    "\n",
    "file <- read_json(here(\"coverage_files/dna_236574_A20_1.json\"), simplifyVector = T)\n",
    "#file$analyses$`testsuites/phylo` #gives result of testsuites/phylo \n",
    "#file$info$fastq #fastq gives name\n",
    "#file$coverages #coverage of each snp\n",
    "\n",
    "cov_files <- dir(here(\"coverage_files\"), pattern=\"*.json\")\n",
    "#data <- cov_files %>%\n",
    "#       map_df(~fromJSON(file.path(here(\"coverage_files\"), .), flatten = TRUE))\n",
    "\n",
    "kvarq_df <- NULL\n",
    "kvarq_df <- as.data.frame(matrix(\"\", ncol=2))\n",
    "colnames(kvarq_df) <- c(\"file\", \"lineage\")\n",
    "\n",
    "filenames <- NULL\n",
    "lineages <- NULL\n",
    "\n",
    "for (i in 1:length(cov_files)){\n",
    "  fasta <- read_json(here(\"coverage_files\", cov_files[i]), simplifyVector = T)\n",
    "  filenames <- c(filenames, fasta$info$fastq[[1]])\n",
    "  lineages  <- c(lineages, fasta$analyses$`testsuites/phylo`)\n",
    "}\n",
    "kvarq_df <- tibble(samples=filenames, analysis=lineages)\n",
    "\n",
    "# simplify sample names\n",
    "kvarq_df$samples <- sub(\"fastq/dna_\", \"\", kvarq_df$samples)\n",
    "\n",
    "kvarq_df$samples <- sub(\".fastq.gz\", \"\", kvarq_df$samples)\n",
    "kvarq_df$samples <- sub(\".fastq\", \"\", kvarq_df$samples)\n",
    "                        \n",
    "\n",
    "# Split name column into firstname and last name\n",
    "kvarq_df[c('Primary_lineage', 'Secondary_lineage')] <- str_split_fixed(kvarq_df$analysis, '/', 2)\n",
    "kvarq_df[c('Secondary_lineage', 'Tertiary_lineage')] <- str_split_fixed(kvarq_df$Secondary_lineage, '-', 2)\n",
    "View(kvarq_df)\n",
    "\n",
    "#write_tsv(kvarq_df, here(\"mbovis_lineage.tsv\"), col_names=T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the samples from lineage 1.8 (1.8.2 specifically)\n",
    "1.8.2 is subdivided from Eu1 clonal complex. INcludes isolates from France and continental Europe, Africa, including Madagascar.\n",
    "\n",
    "3 samples are 1.7.1, though these had more 'mixed coverage'. 1.7.1 is an Eu2 clonal complex subgroup within 1.7. Occurs in Western Europe, America and Southern Africa. Few genomes known\n",
    "\n",
    "(lineage info from Zwyer et al, 2021)\n",
    "\n",
    "## vcf parsing\n",
    "\n",
    "Look at vcf files from each sample to identify genes with snps and degree of predicted impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "data <- read_tsv(here(\"mbovis_vcf_files/dna_236574_A20.annot.vcf\"), comment = \"##\", col_names=TRUE)\n",
    "\n",
    " #INFO_cols <- c(\"Functional annotations: 'Allele | Annotation | Annotation_Impact | Gene_Name | Gene_ID | Feature_Type | Feature_ID | Transcript_BioType | Rank | HGVS.c | HGVS.p | cDNA.pos / cDNA.length | CDS.pos / CDS.length | AA.pos / AA.length | Distance | ERRORS / WARNINGS / INFO' \">\n",
    "##INFO=<ID=LOF,Number=.,Type=String,Description=\"Predicted loss of function effects for this variant. Format: 'Gene_Name | Gene_ID | Number_of_transcripts_in_gene | Percent_of_transcripts_affected'\">\n",
    "##INFO=<ID=NMD,Number=.,Type=String,Description=\"Predicted nonsense mediated decay effects for this variant. Format: 'Gene_Name | Gene_ID | Number_of_transcripts_in_gene | Percent_of_transcripts_affected'\">\n",
    "\n",
    "a <- str_split(data$INFO[1], \"\\\\|\")\n",
    "a\n",
    "length(data$INFO)\n",
    "info_data <- str_split(data$INFO, \"\\\\|\")\n",
    "ncol_info <- length(info_data[[1]])\n",
    "head(info_data)\n",
    "my_cols <- c(\"Allele\", \"Annotation\",\"Annotation_Impact\",\"Gene_Name\",\"Gene_ID\",\"Feature_Type\",\"Feature_ID\", \"Transcript_BioType\" ,\"Rank\",\"HGVS.c\",\"HGVS.p\",\"cDNA.pos/cDNA.length\",\"CDS.pos/CDS.length\",\"AA.pos_AA.length\",\"Distance\",\"errors/warnings/info\")\n",
    "vcf_data <- as_tibble(lapply(1:ncol_info,function(i)sapply(info_data,\"[\",i)), .name_repair=\"minimal\")\n",
    "colnames(vcf_data) <- my_cols\n",
    "\n",
    "#Look at particular categories of mutations\n",
    "vcf_data %>% filter(Annotation_Impact==\"HIGH\")\n",
    "\n",
    "#save data as spreadsheet\n",
    "#write_excel_csv(vcf_data, here(\"mbovis_vcf_files/vcf_snpeff_results.csv\"))\n",
    "\n",
    "filenames <- dir(here(\"mbovis_vcf_files\"), pattern=\"*.annot.vcf\")\n",
    "for (i in 1:length(filenames)){\n",
    "  sample <- substr(filenames[i], 1, 14)\n",
    "  data <- read_tsv(here(\"mbovis_vcf_files\", filenames[i]), comment = \"##\",\n",
    "                   col_names=TRUE, show_col_types = F)\n",
    "  info_data <- str_split(data$INFO, \"\\\\|\")\n",
    "  vcf_data <- as_tibble(lapply(1:ncol_info,function(i)sapply(info_data,\"[\",i)),\n",
    "                        .name_repair=\"minimal\")\n",
    "  colnames(vcf_data) <- my_cols\n",
    "  res_filename <- paste(sample, \"snpEff_res.csv\", sep=\"_\")\n",
    "  write_excel_csv(vcf_data, here(\"mbovis_vcf_files/vcf_snpeff_res\", res_filename))\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make nested dataframe of all results from all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(purrr)\n",
    "# all in one nested dataframe with file information\n",
    "\n",
    "data_path <- here(\"mbovis_vcf_files/vcf_snpeff_res\")\n",
    "\n",
    "filenames <- dir(data_path, pattern=\"*.csv\")\n",
    "df1 <- data_frame(filename=filenames) %>%\n",
    "  mutate(file_contents = map(filenames,          # read files into\n",
    "           ~ read_csv(file.path(data_path, .), col_names = T, show_col_types = F)) # a new data column\n",
    "        )  \n",
    "colnames(unnest(df1, cols=2))\n",
    "# rename with sample name instead of filename \n",
    "df1[[1]]<-sub(\"_snpEff_res.csv\", \"\", df1[[1]])\n",
    "\n",
    "# list of nested dfs\n",
    "unnest(df1, cols = 1)\n",
    "\n",
    "# all unnested\n",
    "all_data <- unnest(df1, cols=2)\n",
    "\n",
    "# get particular sample\n",
    "all_data %>%\n",
    "  filter(filename==\"dna_236574_A20\")\n",
    "\n",
    "# all mutations of particular gene\n",
    "all_data %>%\n",
    "  filter(Gene_Name==\"pks3\") %>%\n",
    "  group_by(filename) \n",
    "\n",
    "#save data \n",
    "#saveRDS(all_data, here(\"R_data/snpEff_annot_vcf.RData\"))\n",
    "\n",
    "## look for snps in phoP and phoR and intergenic regions adjacent\n",
    "\n",
    "snp_data %>% filter(grepl(\"Mb0780\", Gene_ID))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No snps in phoP and phoR or adjacent intergenic regions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
